<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pytorch入门学习1</title>
    <link href="/2020/11/04/pytorch1/"/>
    <url>/2020/11/04/pytorch1/</url>
    
    <content type="html"><![CDATA[<h2 id="pytorch入门学习（一）"><a href="#pytorch入门学习（一）" class="headerlink" title="pytorch入门学习（一）"></a>pytorch入门学习（一）</h2><h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><pre><code class="hljs elm"><span class="hljs-keyword">import</span> torch</code></pre><p>构建一个未初始化的5*3矩阵</p><pre><code class="hljs angelscript">x=torch.empty(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)</code></pre><p>构建一个随机初始化的矩阵</p><pre><code class="hljs angelscript">x=torch.rand(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)</code></pre><p>构建一个全部为0，类型为long的矩阵</p><pre><code class="hljs angelscript">x=torch.zeros(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,dtype=long)</code></pre><p>从数据直接构建tensor</p><pre><code class="hljs apache"><span class="hljs-attribute">x</span>=torch.tensor([<span class="hljs-number">5</span>.<span class="hljs-number">5</span>,<span class="hljs-number">3</span>])//结果显示为tensor([<span class="hljs-number">5</span>.<span class="hljs-number">5000</span>,<span class="hljs-number">3</span>.<span class="hljs-number">0000</span>])</code></pre><p>也可以从一个已有的tensor构建一个tensor.这些办法会重用原来tensor的特征，例如数据类型，除非提供新的数据。</p><pre><code class="hljs reasonml">x=x.<span class="hljs-keyword">new</span><span class="hljs-constructor">_ones(5,3,<span class="hljs-params">dtype</span>=<span class="hljs-params">torch</span>.<span class="hljs-params">double</span>)</span></code></pre><p>得到tensor的形状：</p><pre><code class="hljs awk">x.size()x.shape<span class="hljs-regexp">//</span>两者效果一样<span class="hljs-regexp">//</span>torch.size()返回的是一个tuple</code></pre><h3 id="operation"><a href="#operation" class="headerlink" title="operation"></a>operation</h3><pre><code class="hljs gml"><span class="hljs-symbol">y</span>=torch.rand(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)<span class="hljs-comment">//方法一</span><span class="hljs-symbol">x</span>+<span class="hljs-symbol">y</span><span class="hljs-comment">//方法二</span>torch.add(<span class="hljs-symbol">x</span>,<span class="hljs-symbol">y</span>)</code></pre><p>也可以把输出当成变量</p><pre><code class="hljs routeros"><span class="hljs-attribute">result</span>=torch.empty(5,3)torch.<span class="hljs-builtin-name">add</span>(x,y,<span class="hljs-attribute">out</span>=result)</code></pre><p>加法in-place add x to y</p><pre><code class="hljs gml"><span class="hljs-symbol">y</span>.add_(<span class="hljs-symbol">x</span>)</code></pre><p>使用标准的Numpy类似的牵引操作</p><pre><code class="hljs angelscript">x[:,<span class="hljs-number">1</span>]<span class="hljs-comment">//取出第2列数据</span>x[:,<span class="hljs-number">1</span>:]<span class="hljs-comment">//取出第二列第三列...的数据</span>x[<span class="hljs-number">1</span>:,<span class="hljs-number">1</span>:]<span class="hljs-comment">//取出第二行...的数据，在取出第二列...的数据</span></code></pre><p>改变大小：如果你想改变一个tensor的大小或者形状</p><pre><code class="hljs apache"><span class="hljs-attribute">x</span>=torch.rand(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<span class="hljs-attribute">y</span> = x.view(<span class="hljs-number">16</span>)//y变成了一行<span class="hljs-number">16</span>个<span class="hljs-attribute">z</span> = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)//设置维度为-<span class="hljs-number">1</span>，则系统自动会帮助你算出，例子中自动算出是<span class="hljs-number">2</span></code></pre><p>如果你有一个元素 tensor ，使用 .item() 来获得这个 value</p><h3 id="Numpy和Tensor之间的转化"><a href="#Numpy和Tensor之间的转化" class="headerlink" title="Numpy和Tensor之间的转化"></a>Numpy和Tensor之间的转化</h3><p>在Torch Tensor和NumPy array会<strong>共享内存</strong>，所以改变其中一项也会改变另外一项</p><p>把Torch Tensor转变成Numpy Array</p><pre><code class="hljs stylus">a=torch.ones(<span class="hljs-number">5</span>)<span class="hljs-comment">//tensor</span><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a)</span></span>b=<span class="hljs-selector-tag">a</span>.numpy()<span class="hljs-comment">//tensor转成numpy</span><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><span class="hljs-selector-tag">b</span>[<span class="hljs-number">1</span>]=<span class="hljs-number">2</span><span class="hljs-comment">//修改下标为1的值,共享了内存空间</span></code></pre><p>把Numpy Array转变成Torch Tensor</p><pre><code class="hljs livecodeserver"><span class="hljs-keyword">a</span>=np.ones(<span class="hljs-number">5</span>)b=torch.from_numpy(<span class="hljs-keyword">a</span><span class="hljs-comment">)//转变为tensor</span>np.<span class="hljs-built_in">add</span>(<span class="hljs-keyword">a</span>,<span class="hljs-number">1</span>,out=<span class="hljs-keyword">a</span>)print(<span class="hljs-keyword">a</span><span class="hljs-comment">)//[2.,2.,2.,2.,2.]</span></code></pre><h3 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h3><p>使用.to方法，Tensor可以被移动到别的device上。尤其是在大运算面前，使用GPU运算效率会高很多</p><pre><code class="hljs gml"><span class="hljs-keyword">if</span> torch.cuda.is_available():    device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)    <span class="hljs-symbol">y</span>=torch.ones_like(<span class="hljs-symbol">x</span>, device=device)    <span class="hljs-symbol">x</span>=<span class="hljs-symbol">x</span>.to(device)    z=<span class="hljs-symbol">x</span>+<span class="hljs-symbol">y</span> <span class="hljs-comment">//z在cuda(GPU)上运算会提速</span>    <span class="hljs-comment">//使用numpy的时候必须为CPU,在GPU状态下必须先转为CPU</span>    <span class="hljs-symbol">y</span>.to(<span class="hljs-string">&quot;cpu&quot;</span>).data.numpy()    <span class="hljs-comment">//模型转为GPU上运行</span>    model=model.cuda()</code></pre><p>简单实现一个深度模型算法</p><pre><code class="hljs nix">from __future__ <span class="hljs-built_in">import</span> print_function<span class="hljs-built_in">import</span> torch<span class="hljs-built_in">import</span> numpy as npN,D_in,H,<span class="hljs-attr">D_out=64,1000,100,10</span><span class="hljs-attr">x=torch.randn(N,D_in,requires_grad=True)</span><span class="hljs-attr">y=torch.randn(N,D_out,requires_grad=True)</span><span class="hljs-attr">w1=torch.randn(D_in,H,requires_grad=True)</span><span class="hljs-attr">w2=torch.randn(H,D_out,requires_grad=True)</span><span class="hljs-attr">learning_rate=1e-6</span>for it <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):    <span class="hljs-comment">#fowward pass</span>    <span class="hljs-attr">y_pred=x.mm(w1).clamp(min=0).mm(w2)</span>    <span class="hljs-comment">#compute loss</span>    <span class="hljs-attr">loss=(y_pred-y).pow(2).sum()</span>    print(it,loss.item())    <span class="hljs-comment">#backward pass</span>    loss.backward()    <span class="hljs-comment">#update weights of w1 and w2</span>    <span class="hljs-keyword">with</span> torch.no_grad():        w1 <span class="hljs-attr">-=</span> learning_rate*w1.grad        w2 <span class="hljs-attr">-=</span> learning_rate*w2.grad        w1.grad.zero_()        w2.grad.zero_()</code></pre><p>使用model改进一下</p><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> __future__ import print_functionimport torchimport numpy as npN,D_in,H,<span class="hljs-attribute">D_out</span>=64,1000,100,10<span class="hljs-attribute">x</span>=torch.randn(N,D_in,requires_grad=True)<span class="hljs-attribute">y</span>=torch.randn(N,D_out,requires_grad=True)<span class="hljs-comment">#torch方法2 使用model</span><span class="hljs-attribute">model</span>=torch.nn.Sequential(    torch.nn.Linear(D_in,H,<span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>),    torch.nn.ReLU(),    torch.nn.Linear(H,D_out,<span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>),)<span class="hljs-comment">#用于优化模型的normal初始化</span>torch.nn.init.normal_(model[0].weight)torch.nn.init.normal_(model[2].weight)<span class="hljs-comment">#在GPU上运行</span><span class="hljs-comment">#model =model.cuda()</span><span class="hljs-attribute">learning_rate</span>=1e-6<span class="hljs-comment"># 用于优化</span>optimizer =torch.optim.Adam(model.parameters(),<span class="hljs-attribute">lr</span>=learning_rate)<span class="hljs-attribute">loss_fn</span>=torch.nn.MSELoss(reduction=&#x27;sum&#x27;)<span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> range(500):    #fowward pass    <span class="hljs-attribute">y_pred</span>=model(x)    #compute loss    <span class="hljs-attribute">loss</span>=loss_fn(y_pred,y)    <span class="hljs-builtin-name">print</span>(it,loss.item())    optimizer.zero_grad()    #backward pass    loss.backward()    optimizer.<span class="hljs-keyword">step</span>()    #update weights of w1 <span class="hljs-keyword">and</span> w2    with torch.no_grad():        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():            <span class="hljs-attribute">param-</span>=learning_rate*param.grad    model.zero_grad()</code></pre><p>使用class</p><pre><code class="hljs haskell"><span class="hljs-title">from</span> __future__ <span class="hljs-keyword">import</span> print_function<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-type">N</span>,<span class="hljs-type">D_in</span>,<span class="hljs-type">H</span>,<span class="hljs-type">D_out</span>=<span class="hljs-number">64</span>,<span class="hljs-number">1000</span>,<span class="hljs-number">100</span>,<span class="hljs-number">10</span><span class="hljs-title">x</span>=torch.randn(<span class="hljs-type">N</span>,<span class="hljs-type">D_in</span>,requires_grad=<span class="hljs-type">True</span>)<span class="hljs-title">y</span>=torch.randn(<span class="hljs-type">N</span>,<span class="hljs-type">D_out</span>,requires_grad=<span class="hljs-type">True</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">TwoLayerNet</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>,<span class="hljs-type">D_in</span>,<span class="hljs-type">H</span>,<span class="hljs-type">D_out</span>):</span><span class="hljs-class">        super(<span class="hljs-type">TwoLayerNet</span>,<span class="hljs-title">self</span>).__init__()</span><span class="hljs-class">        #define the model architecture</span><span class="hljs-class">        self.linear1=torch.nn.<span class="hljs-type">Linear</span>(<span class="hljs-type">D_in</span>,<span class="hljs-type">H</span>,<span class="hljs-title">bias</span>=<span class="hljs-type">False</span>)</span><span class="hljs-class">        self.linear2=torch.nn.<span class="hljs-type">Linear</span>(<span class="hljs-type">H</span>,<span class="hljs-type">D_out</span>, <span class="hljs-title">bias</span>=<span class="hljs-type">False</span>)</span><span class="hljs-class"></span><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><span class="hljs-class">        y_pred=self.linear2(<span class="hljs-title">self</span>.<span class="hljs-title">linear1</span>(<span class="hljs-title">x</span>).clamp(<span class="hljs-title">min</span>=0))</span><span class="hljs-class">        return y_pred</span><span class="hljs-class"></span><span class="hljs-class">model=<span class="hljs-type">TwoLayerNet</span>(<span class="hljs-type">D_in</span>,<span class="hljs-type">H</span>,<span class="hljs-type">D_out</span>)</span><span class="hljs-class">loss_fn=torch.nn.<span class="hljs-type">MSELoss</span>(<span class="hljs-title">reduction</span>=&#x27;<span class="hljs-title">sum&#x27;</span>)</span><span class="hljs-class">learning_rate=1e-4</span><span class="hljs-class"># 用于优化</span><span class="hljs-class">optimizer =torch.optim.<span class="hljs-type">Adam</span>(<span class="hljs-title">model</span>.<span class="hljs-title">parameters</span>(),lr=learning_rate)</span><span class="hljs-class"></span><span class="hljs-class"></span><span class="hljs-class">for it in range(500):</span><span class="hljs-class">    #fowward pass</span><span class="hljs-class">    y_pred=model.forward(<span class="hljs-title">x</span>)</span><span class="hljs-class"></span><span class="hljs-class">    #compute loss</span><span class="hljs-class">    loss=loss_fn(<span class="hljs-title">y_pred</span>,<span class="hljs-title">y</span>)</span><span class="hljs-class">    print(<span class="hljs-title">it</span>,<span class="hljs-title">loss</span>.<span class="hljs-title">item</span>())</span><span class="hljs-class"></span><span class="hljs-class">    optimizer.zero_grad()</span><span class="hljs-class">    #backward pass</span><span class="hljs-class">    loss.backward()</span><span class="hljs-class"></span><span class="hljs-class">    optimizer.step()</span><span class="hljs-class"></span><span class="hljs-class">    #update weights of w1 and w2</span><span class="hljs-class">    with torch.no_grad():</span><span class="hljs-class">        for param in model.parameters():</span><span class="hljs-class">            param-=learning_rate*param.grad</span><span class="hljs-class"></span><span class="hljs-class">    model.zero_grad()</span></code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>算法篇之NP问题</title>
    <link href="/2020/10/19/narithmetic1Np/"/>
    <url>/2020/10/19/narithmetic1Np/</url>
    
    <content type="html"><![CDATA[<h2 id="P问题，NP问题，NPC问题，NP难问题"><a href="#P问题，NP问题，NPC问题，NP难问题" class="headerlink" title="P问题，NP问题，NPC问题，NP难问题"></a>P问题，NP问题，NPC问题，NP难问题</h2><p><strong>时间复杂度</strong>表示当问题规模扩大后，程序需要的时间长度增长得有多快，譬如这个数据的规模扩大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，甚至慢了数万倍。复杂度被分为两种级别，一种是O(1),O(log(n)),O(n^a),这种被称为是多项式级的复杂度，另外一种是O(a^n),O(n!)是非多项式级的复杂度，其复杂度计算机往往不能承受。</p><p>所有的问题都可以找到复杂度为多项式级的算法？答案是否定的，有些问题甚至找不到一个正确的算法，这被称为不可解问题。</p><p><strong>P问题</strong>：如果一个问题可以找到一个能在多项式的时间里解决它的算法，那这问题属于P问题</p><p><strong>NP问题</strong>：可以在多项式的时间里面验证一个解的问题。所有的P类问题都是NP问题。NP的另外一个定义是，可以在多项式的时间里才出一个解的问题，例如从起点到终点是否有条小于100单位长度的路线，结果我随便画了一条线，发现路径长度为98，符合要求。在这个题中，找一个解很困难，但是验证一个解很容易，验证一个解只需要O(n)的时间复杂度。</p><p>约化（归约）问题A可以约化问题B的含义是，可以用问题B的解法解决问题A，例如求解一元一次方程和解一元二次方程，前者可以约化为后者，我们可以写出两个程序分别对应两个问题，那么我们能找到一个规则，按照这个规则把解一元一次方程程序的数据数据变一下，用在解一元二次方程程序上，两个程序能得到一样的结果。这个规则即是两个方程系数项对应不变，一元二次方程的二次系数为0.按照这个规则把前一个问题转换为后一个问题了。</p><p>问题A约化为问题B有一个直观意义：B的时间复杂度高于或者等于A的时间复杂度。</p><p>约化具有传递性：问题A约化为问题B，问题B约化为问题C则问题A约化为问题C</p><p>约化的标准概念现在就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按照这个法则变换成B的输入，使两程序的输出相同，那么我们说，问题A可以约化为问题B。</p><p>从约化的定义中可以看到，一个问题约化为另外一个问题，时间复杂度增加了，问题的应用范围也增大了，如果不断的约化上去，能找到一个通吃所有NP问题的超级NP问题，只要解决了这个问题，那么所有NP问题都解决了。这种问题不只是一个，可以有很多个，这一类问题就是<strong>NPC问题</strong>（NP-完全问题）</p><p>NPC问题的定义：要求同时满足两个要求，首先是一个NP问题，然后所有的NP问题都能约化到他。</p><p><strong>NP-Hard问题</strong>：他满足NPC定义的第二条但不一定要满足第一条</p><p>本文参考自<a href="http://www.matrix67.com/blog/archives/105">http://www.matrix67.com/blog/archives/105</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>边缘计算综述</title>
    <link href="/2020/10/17/EdgeComputing1/"/>
    <url>/2020/10/17/EdgeComputing1/</url>
    
    <content type="html"><![CDATA[<h2 id="边缘计算综述"><a href="#边缘计算综述" class="headerlink" title="边缘计算综述"></a>边缘计算综述</h2><ul><li><p>边缘计算模型是指在网络边缘执行计算的一种新型计算模型。</p></li><li><p>边缘计算中边缘设备具有执行计算和数据分析的处理能力，将原有云计算模型执行的部分或全部计算任务迁移到网络边缘设备上，降低云服务器的计算负载，减少网络带宽压力，提高数据的处理效率。</p></li><li><p>边缘计算是对云的补充。</p></li></ul><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>边缘计算是一种新型计算模式，通过在靠近物或者数据源头的网络边缘侧，为应用提供融合计算、存储和网络等资源。同时，边缘计算通过在网络边缘侧通过这些资源，满足行业在敏捷联接、实时业务、数据优 化、应用智能、安全与隐私保护等需求</p><h3 id="边缘计算的体系架构"><a href="#边缘计算的体系架构" class="headerlink" title="边缘计算的体系架构"></a>边缘计算的体系架构</h3><p>包括为终端层、边缘层、云层</p><ul><li>终端层：最接近终端用户，由各种物联网设备(传感器，智能手机，智能车辆，智能卡，读卡器…)组成。为了延长终端设备提供服务的时间，避免在终端设备上运行复杂的计算任务。因此只将终端设备负责收集原始数据，上传到上层进行计算和存储。终端层连接上一层主要通过蜂窝网络。</li><li>边缘层：有大量的边缘节点（路由器、网关、交换机、接入点、基站）组成，对终端设备上传的数据进行计算和存储，可以为运行对延迟比较敏感的应用提供实时性的服务。边缘节点也可以对收集的数据进行预处理，再把数据上传到云端，从而减少核心网络的传输量。边缘层连接上层主要通过因特网</li><li>云层：由多个高性能服务器和存储设备组成，具有强大的计算和存储功能，可以执行复杂的计算任务。云模块通过控制策略可以有效地管理和调度边缘节点和云计算中心。</li></ul><h3 id="边缘计算的范例"><a href="#边缘计算的范例" class="headerlink" title="边缘计算的范例"></a>边缘计算的范例</h3><p>如雾计算、移动边缘计算，他们在动机、节点设备、节点位置等上与边缘计算范例类似。协同边缘计算是一种新的计算范例，<strong>它使用边缘计算和路由器的网状网络来实现网络内的分布式决策。决策实在网络内部通过在边缘设备之间共享数据和计算而不是将所有数据发送到集中式数据服务器来完成的</strong>。通常的方式是集中计算，网关的边缘设备仅仅收集数据并将数据发送到服务器来处理。</p><h3 id="边缘计算优势"><a href="#边缘计算优势" class="headerlink" title="边缘计算优势"></a>边缘计算优势</h3><ol><li>实时数据处理和分析：云计算中心的计算任务部分或者全部迁移到网络边缘，提高了数据传输的性能，保证了处理的时效性，同时也降低了云中心的计算负载。</li><li>安全性高：集中式的云计算模型，容易受到分布式拒绝服务供给和断电的影响，边缘计算同时也降低了发生单点故障的可能性。</li><li>保护隐私数据：边缘计算模型是在本地设备上处理更多数据而不是上传到云中心。即使设备收到攻击，它也指挥包含本地收集的数据，而不是受损的云计算中心。</li><li>可扩展性：边缘计算提供更便宜的可扩展路径，允许公司通过物联网设备和边缘数据中心的组合来扩展其计算能力。使用具有处理能力的物联网设备还可以降低扩展成本，添加新的设备都不会对网络产生大量带宽需求。</li><li>位置感知：边缘分布式设备利用低级信令进行信息共享。边缘计算模型从本地接入网络内的边缘设备接收信息以发现设备的位置。例如导航，终端设备根据自己的实时位置把相关位置信息和数据交给边缘节点来进行处理，边缘节点基于现有数据进行判断和决策。</li><li>低流量：本地设备收集的数据可以进行本地计算分析，或者在本地进行数据的预处理，减少进行核心网的流量。</li></ol><h3 id="边缘计算应用"><a href="#边缘计算应用" class="headerlink" title="边缘计算应用"></a>边缘计算应用</h3><ol><li>保健医疗</li><li>视频分析</li><li>车辆互联</li><li>移动大数据分析</li><li>智能建筑控制</li><li>海洋监测控制</li><li>智能家居</li><li>智慧城市</li></ol><h3 id="边缘计算现状和关键技术"><a href="#边缘计算现状和关键技术" class="headerlink" title="边缘计算现状和关键技术"></a>边缘计算现状和关键技术</h3><ol><li>计算卸载：终端设备将部分或全部的计算任务卸载到资源丰富的边缘服务器，以解决终端设备在资源存储、计算性能以及能效等房买你存在的不足。主要技术是卸载策略，卸载策略主要解决的是移动端如何卸载计算任务、卸载多少以及卸载什么的问题。根据卸载决策的优化目标将计算卸载分为降低时延为目的、降低能耗为目的、权衡能耗和时延为目的的三种类型。</li><li>移动性管理：云计算模式对应用移动性的支持则是服务器位置固定，数据通过网络传输到服务器。在边缘计算中，移动管理是一种新模式，主要涉及两个问题，资源发现。即用户在移动的过程中需要快速发现周围可以利用的资源，并选择最合适的资源，边缘计算的资源发现需要适应异构的资源环境，还需要保证资源发现的速度，才能使应用不间断的为用户提供服务；资源切换，用户移动时，移动应用使用的计算资源可能会在多个设备间切换，保证服务连续性是边缘计算研究的一个重点。</li><li>其他：网络控制、内存缓存、内容自适应、数据聚合、安全卸载。</li></ol><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><ol><li>优化边缘计算性能：在边缘计算架构中，不同层次的边缘服务器所拥有的计算能力不同，负载分配将成为一个重要的问题。用户需求、延时、带宽、能耗以及成本是决定负载分配策略的关键指标。针对不同的工作负载，应设置指标的权重和优先级，以便系统选择最优分配策略。</li><li>安全性：在边缘计算架构中，在数据源的附近进行计算是保护隐私和数据安全的一种较合适的办法。但由于网络边缘设备的资源有限，现有数据安全不完全适用于边缘计算架构。网络边缘高度动态的环境也会使网络更加易受攻击。</li><li>互操作性：互操作性是边缘计算架构能够大规模落地的关键。不同设备商需要制定相关的标准规范和通用的协作协议，实现异构边缘设备和系统之间的互操作性。</li><li>智能边缘操作管理服务：在物联网环境中需要满足识别服务优先级，灵活可拓展和复杂环境下的隔离线。在传感器数据和通信不可靠的情况下，系统如何通过利用多维参考数据源和历史数据记录，提供可靠的服务是目前的关注问题。</li></ol><p>[1]丁春涛,曹建农,杨磊,王尚广.边缘计算综述:应用、现状及挑战[J].中兴通讯技术,2019,25(03):2-7.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>go interface详解</title>
    <link href="/2020/09/22/go-interface/"/>
    <url>/2020/09/22/go-interface/</url>
    
    <content type="html"><![CDATA[<h3 id="1-interface是一种类型"><a href="#1-interface是一种类型" class="headerlink" title="1.interface是一种类型"></a>1.interface是一种类型</h3><p>​    interface是一种具有一组方法的类型，一个类型实现了interface的所有方法，我们说类型实现了该inteface</p><h3 id="2-interface变量存储值"><a href="#2-interface变量存储值" class="headerlink" title="2.interface变量存储值"></a>2.interface变量存储值</h3>  <pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><span class="hljs-keyword">type</span> Human <span class="hljs-keyword">struct</span> &#123;    name  <span class="hljs-keyword">string</span>    age   <span class="hljs-keyword">int</span>    phone <span class="hljs-keyword">string</span>&#125;<span class="hljs-keyword">type</span> Student <span class="hljs-keyword">struct</span> &#123;    Human    school <span class="hljs-keyword">string</span>    loan   <span class="hljs-keyword">float32</span>&#125;<span class="hljs-keyword">type</span> Employee <span class="hljs-keyword">struct</span> &#123;    Human    company <span class="hljs-keyword">string</span>    money   <span class="hljs-keyword">float32</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(h Human)</span> <span class="hljs-title">SayHi</span><span class="hljs-params">()</span></span> &#123;    fmt.Printf(<span class="hljs-string">&quot;Hi, I am %s you can call me on %s\n&quot;</span>, h.name, h.phone)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(h Human)</span> <span class="hljs-title">Sing</span><span class="hljs-params">(lyrics <span class="hljs-keyword">string</span>)</span></span> &#123;    fmt.Println(<span class="hljs-string">&quot;La la la la...&quot;</span>, lyrics)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(e Employee)</span> <span class="hljs-title">SayHi</span><span class="hljs-params">()</span></span> &#123;    fmt.Printf(<span class="hljs-string">&quot;Hi, I am %s, I work at %s. Call me on %s\n&quot;</span>, e.name, e.company, e.phone)&#125;<span class="hljs-keyword">type</span> Men <span class="hljs-keyword">interface</span> &#123;    SayHi()    Sing(lyrics <span class="hljs-keyword">string</span>)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;    mike := Student&#123;Human&#123;<span class="hljs-string">&quot;Mike&quot;</span>, <span class="hljs-number">25</span>, <span class="hljs-string">&quot;222-222-XXX&quot;</span>&#125;, <span class="hljs-string">&quot;MIT&quot;</span>, <span class="hljs-number">0.00</span>&#125;    paul := Student&#123;Human&#123;<span class="hljs-string">&quot;Paul&quot;</span>, <span class="hljs-number">26</span>, <span class="hljs-string">&quot;111-222-XXX&quot;</span>&#125;, <span class="hljs-string">&quot;Harvard&quot;</span>, <span class="hljs-number">100</span>&#125;    sam := Employee&#123;Human&#123;<span class="hljs-string">&quot;Sam&quot;</span>, <span class="hljs-number">36</span>, <span class="hljs-string">&quot;444-222-XXX&quot;</span>&#125;, <span class="hljs-string">&quot;Golang Inc.&quot;</span>, <span class="hljs-number">1000</span>&#125;    tom := Employee&#123;Human&#123;<span class="hljs-string">&quot;Tom&quot;</span>, <span class="hljs-number">37</span>, <span class="hljs-string">&quot;222-444-XXX&quot;</span>&#125;, <span class="hljs-string">&quot;Things Ltd.&quot;</span>, <span class="hljs-number">5000</span>&#125;    <span class="hljs-keyword">var</span> m, p, s, t Men = mike, paul, sam, tom    m.SayHi()    p.SayHi()    s.SayHi()    t.SayHi()&#125;</code></pre><p>多种类型实现了某个接口，这些类型的值都可以直接使用interface的变量存储</p><h3 id="3-interface类型断言"><a href="#3-interface类型断言" class="headerlink" title="3.interface类型断言"></a>3.interface类型断言</h3><p>​    em代表interface类型的变量，T代表断言的类型，value是interface变量存储值，ok：bool类型</p><pre><code class="hljs go"><span class="hljs-keyword">if</span> t, ok := i.(*S); ok &#123;    fmt.Println(<span class="hljs-string">&quot;s implements I&quot;</span>, t)&#125;</code></pre><h3 id="4-空interface"><a href="#4-空interface" class="headerlink" title="4.空interface"></a>4.空interface</h3><p>​    空的interface可以接受任何类型作为参数</p><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doSthing</span><span class="hljs-params">(v <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span>&#123;&#125;</code></pre><p>​    参数v可以接受任何类型，但是函数被调用时在函数内部不代表任何类型，因为go在执行时传递给函数的任何类型都自动转换成interface{}</p><p>​    那么一个interface{的slice是否可以接受任何类型的slice？</p><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printAll</span><span class="hljs-params">(vals []<span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> &#123;     <span class="hljs-keyword">for</span> _, val := <span class="hljs-keyword">range</span> vals &#123;        fmt.Println(val)    &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;    names := []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">&quot;stanley&quot;</span>, <span class="hljs-string">&quot;david&quot;</span>, <span class="hljs-string">&quot;oscar&quot;</span>&#125;    printAll(names)&#125;</code></pre><p>​    执行之后会报错，因为interface{}会占用两个字长的存储空间，一个是自身的method数据，一个是指向其存储值的指针，因而slice []interface的长度是固定的<code>N*2</code>,但是[]T的长度是 <code>N*sizeof(T)</code>，两者的实际存储值是有区别的。</p><p>​    但是我们也能手动转换</p><pre><code class="hljs go"><span class="hljs-keyword">var</span> dataSlice []<span class="hljs-keyword">int</span> = foo()<span class="hljs-keyword">var</span> interfaceSlice []<span class="hljs-keyword">interface</span>&#123;&#125; = <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">interface</span>&#123;&#125;, <span class="hljs-built_in">len</span>(dataSlice))<span class="hljs-keyword">for</span> i, d := <span class="hljs-keyword">range</span> dataSlice &#123;    interfaceSlice[i] = d&#125;</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>4_opengl</title>
    <link href="/2020/08/18/4-opengl/"/>
    <url>/2020/08/18/4-opengl/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>UEFI模式下安装Ubuntu系统</title>
    <link href="/2020/08/18/3-ueei-ubuntudownload/"/>
    <url>/2020/08/18/3-ueei-ubuntudownload/</url>
    
    <content type="html"><![CDATA[<p>有的电脑不适合这种方法，可以百度查找legacy模式安装ubuntu方法基本差不多，个别地方的设置不同。</p><ol><li><p>分区（50GB）</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVpgH.md.jpg"></p><p>图片说这个20G的分区还不能用，其实这样就可以了（因为这个方法是用EFI模式下安装ubuntu），就留着这20G未分配就好了。实际操作中我们还要将这20G改成50G，也就是分配出50G的空间。<br>对分区还不大懂得，可以百度，百度中有更详细的步骤。</p></li><li><p>关闭电脑的快速启动</p><p>windows设置-&gt;系统-&gt;电源和睡眠-&gt;其他电源设置-&gt;选择电源按钮的功能-&gt;更改当前不可用的设置-&gt;把快速启动前的√去掉 保存即可。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVS8e.md.png"></p></li><li><p>关闭security boot</p><p>先进入BIOS界面（每个电脑的进入方式不一样，具体百度)</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVPKA.md.png"></p><p>确保BOOT是UEFI模式，然后将Secure Boot改成disable，保存设置然后退出。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlViDI.md.png"></p></li><li><p>插入U盘从U盘启动，安装ubuntu</p><p>然后就是在bios界面下的UEFI模式下选择U盘启动，legacy底下的那些不要选，一定要选UEFI下的USB1-UEFI。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVV58.md.png"></p><p>然后就看到下面的画面，选择第二个</p><p><img src="https://s1.ax1x.com/2020/08/19/dlV9vd.png"></p></li><li><p>系统安装界面设置</p><p>选择中文点击继续，第二界面两个选项都不勾选点继续，第三界面选其他选项，点击继续。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVAVP.md.png"></p><p>这时候就能看到刚刚分区的50G空间(52430MB),点击+号创建分区</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVEUf.md.png"></p><p>按照图片指示创建,创建到最后一个分区，把所有的容量都给他</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVePS.jpg"></p><p>分区创建好了以后，一定要改启动引导，默认的引导是错的，要改成之前我们创建的那个efi系统分区</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVFbt.md.jpg"></p><p>剩下的按照常识点就完事了，密码设置短一些。安装完系统后，拔掉U盘，重启电脑。再进入BTOS界面，把secure boot打开，也就是改成enable。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVm8g.md.png"></p><p>重启电脑，看到这个界面就说明安装成功了。</p><p><img src="https://s1.ax1x.com/2020/08/19/dlVn2Q.md.png"></p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>在注册表添加自定义快速启动程序</title>
    <link href="/2020/08/18/2-register/"/>
    <url>/2020/08/18/2-register/</url>
    
    <content type="html"><![CDATA[<p>进入下图中的地址，添加一个.exe，在项值中输入地址即可</p><p><img src="https://s1.ax1x.com/2020/08/19/dlk7iF.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>FTP的安装与配置(centos)</title>
    <link href="/2020/08/18/1-ftp/"/>
    <url>/2020/08/18/1-ftp/</url>
    
    <content type="html"><![CDATA[<ol><li><p>安装vsftpd，直接yum 安装就可以了</p><pre><code class="hljs cmake">yum <span class="hljs-keyword">install</span> -y vsftpd</code></pre></li><li><p>出现下图表示下载成功</p><img src="https://s1.ax1x.com/2020/08/18/dnfuse.png" style="zoom:150%;" /></li><li><p>相关配置文件</p><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/etc/vsftpd</span><span class="hljs-string">/etc/vsftpd/vsftpd.conf</span> <span class="hljs-string">//</span>主配置文件，核心配置文件<span class="hljs-string">/etc/vsftpd/ftpusers</span> <span class="hljs-string">//</span>黑名单，这个里面的用户不允许访问FTP服务器<span class="hljs-string">/etc/vsftpd/user_list</span> <span class="hljs-string">//</span>白名单，允许访问FTP服务器的用户列表</code></pre></li><li><p>启动服务</p><pre><code class="hljs awk">systemctl enable vsftpd.service <span class="hljs-regexp">//</span>设置开机自启动systemctl start vsftpd.service <span class="hljs-regexp">//</span>启动ftp服务netstat -antup | grep ftp <span class="hljs-regexp">//</span>查看ftp服务端</code></pre><img src="https://s1.ax1x.com/2020/08/18/dnfQZd.png" style="zoom:100%;" /><p>开通ftp服务对应的防火墙端口:21,然后登录ftp</p><img src="https://s1.ax1x.com/2020/08/18/dnfZRK.png" style="zoom:130%;" /></li><li><p>配置本地用戶登录</p><ol><li><p>ftp用户登录</p><pre><code class="hljs nginx"><span class="hljs-attribute">useradd</span> ftptest   <span class="hljs-comment">#创建ftptest用户</span>passwd ftptest    <span class="hljs-comment">#修改ftptest用户密码</span></code></pre><img src="https://s1.ax1x.com/2020/08/18/dnfnMD.png" style="zoom:150%;" /></li><li><p>修改/etc/vsftpd/vsftpd.conf</p><pre><code class="hljs routeros">anonymous <span class="hljs-attribute">enable</span>=<span class="hljs-literal">NO</span><span class="hljs-attribute">local_enable</span>=<span class="hljs-literal">YES</span></code></pre><img src="https://s1.ax1x.com/2020/08/18/dnfVG6.png" style="zoom:150%;" /></li><li><p>通过lftp连接到ftp服务器</p><img src="https://s1.ax1x.com/2020/08/18/dnfKqH.png" style="zoom:150%;" /></li></ol></li></ol><p>​                                                  本文转载自<a href="https://help.aliyun.com/knowledge_detail/60152.html">https://help.aliyun.com/knowledge_detail/60152.html</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
